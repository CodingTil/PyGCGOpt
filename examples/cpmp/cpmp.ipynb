{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ea1623",
   "metadata": {},
   "source": [
    "# Decomposing the Capacitated p-Median Problem\n",
    "The capacitated p-median problem (CPMP) is a known and well-studied problem from the literature. Given\n",
    "$n \\in \\mathbb{N}$ locations, the task is to select $p \\in \\mathbb{N}$ *median* locations with $p \\leq n$ and to\n",
    "assign each location to exactly one median. For any two locations $i,j \\in [n]$, the distance between them is given\n",
    "by $d_{ij} \\in \\mathbb{R}$. The distance between the locations and their assigned medians is minimized. Every\n",
    "location $i \\in [n]$ has a *demand* $q_i \\in \\mathbb{R}$ and a maximum *capacity* $Q_i \\in \\mathbb{R}$. For every\n",
    "selected median, the sum of the demands of the locations assigned to it must not exceed its capacity.\n",
    "\n",
    "The CPMP can be formulated as a MIP. Here is a classical compact formulation of problem:\n",
    "\n",
    "$$\n",
    "\\begin{alignat*}{3}\n",
    "z^{\\star}_{IP}\\,\\,\\,=\\,\\,\\,\\min{} &\\sum_{i=1}^n \\sum_{j=1}^n d_{ij} x_{ij} \\hspace{-2em} &&&&\\\\\n",
    "                     \\text{s. t.} &\\sum_{j=1}^n  x_{ij} &&= 1 \\quad &\\forall i &\\in [n]\\\\\n",
    "                                  &\\sum_{i=1}^{n} q_i x_{ij} &&\\leq Q_j y_j \\quad &\\forall j &\\in [n] \\\\\n",
    "                                  &\\sum_{j=1}^n  y_j &&= p && \\\\\n",
    "                                  &x_{ij} \\in \\{0,1\\}, y_j \\in \\{0,1\\} \\hspace{-6em} &&\\hspace{6em}\\quad& \\forall i,j &\\in [n].\n",
    "\\end{alignat*}\n",
    "$$\n",
    "\n",
    "There are different approaches to solve the CPMP. As it has a structure, we can try to solve using a\n",
    "Branch-Cut-and-Price approach. For that we want to use the `PyGCGOpt` interface to interact with GCG. We will consider three\n",
    "use-cases: (1) The Automatic Mode, (2) Exploring different Decompositions, and (3) Building a custom Decomposition.\n",
    "\n",
    "To follow along with this tutorial interactively, please download the Jupyter notebook from the [examples folder](https://github.com/scipopt/PyGCGOpt/tree/master/examples/cpmp)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bfbf84",
   "metadata": {},
   "source": [
    "## Reading in the Instance\n",
    "The `PyGCGOpt` interface offers two methods to specify a problem. The first is to load the model from a standardized file format (e.g., `.lp` or `.mps`) that is supported by `SCIP` using `Model.readProb()`. Optionally, one can also read in a decomposition from a `.dec` file in the same manner. In this example, we will use the second method: Modeling a problem directly in Python.\n",
    "\n",
    "Execute the following cell which includes a trivial test instance and function to load more instances from a custom `JSON` file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "joint-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_simple_instance():\n",
    "    n = 5\n",
    "    p = 2\n",
    "    d = {0: {0: 0, 1: 25, 2: 46, 3: 43, 4: 30}, 1: {1: 0, 2: 22, 3: 20, 4: 22}, 2: {2: 0, 3: 22, 4: 40}, 3: {3: 0, 4: 22}, 4: {4: 0}}\n",
    "    q = {0: 14, 1: 13, 2: 9, 3: 15, 4: 6}\n",
    "    Q = {i: 33 for i in range(5)}\n",
    "    return n, p, d, q, Q\n",
    "\n",
    "def read_instance_json(path):\n",
    "    with open(path) as f:\n",
    "        instance = json.load(f)\n",
    "    return instance[\"n\"], instance[\"p\"], instance[\"d\"], instance[\"q\"], instance[\"Q\"]\n",
    "\n",
    "n_locations, n_clusters, distances, demands, capacities = read_instance_json(\"instances/p550-01.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ffb130",
   "metadata": {},
   "source": [
    "## Setting up the Model\n",
    "Now, we want to build the model based on the above formulation. Please note that this part is *not* specific to GCG but is *almost* identical to how one would build the same model with `PySCIPOpt`. In fact, the only difference is that we import and instanciate `GCGModel` instead of `Model`. In technical terms, `GCGModel` is a subclass of `Model` and, therefore, you can use all methods of `Model` to build your problem.\n",
    "\n",
    "In order to recreate the model multiple times during this example, we create a method that will return the model. The method also returns the different constraints added to the model grouped by type. This will be important later in use-case 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ffa8aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygcgopt import GCGModel, quicksum\n",
    "\n",
    "def build_model(n_locations, n_clusters, distances, demands, capacities):\n",
    "    m = GCGModel()\n",
    "\n",
    "    m.printVersion()\n",
    "    m.redirectOutput()\n",
    "\n",
    "    m.setMinimize()\n",
    "\n",
    "    x = {}\n",
    "    y = {}\n",
    "\n",
    "    for j in range(1, n_locations + 1):\n",
    "        y[j] = m.addVar(f\"y_{j}\", vtype=\"B\")\n",
    "        for i in range(1, n_locations + 1):\n",
    "            x[i, j] = m.addVar(f\"x_{i}_{j}\", vtype=\"B\", obj=distances[i][j])\n",
    "\n",
    "    # Hold different constraint types\n",
    "    conss_assignment = []\n",
    "    conss_capacity = []\n",
    "    cons_pmedian = None\n",
    "\n",
    "    # Create the assignment constraints\n",
    "    for i in range(1, n_locations + 1):\n",
    "        conss_assignment.append(\n",
    "            m.addCons(quicksum(x[i, j] for j in range(1, n_locations + 1)) == 1)\n",
    "        )\n",
    "\n",
    "    # Create the capacity constraints\n",
    "    for j in range(1, n_locations + 1):\n",
    "        conss_capacity.append(\n",
    "            m.addCons(quicksum(demands[i] * x[i, j] for i in range(1, n_locations + 1)) <= capacities[j] * y[j])\n",
    "        )\n",
    "\n",
    "    # Create the p-median constraint\n",
    "    cons_pmedian = m.addCons(quicksum(y[j] for j in range(1, n_locations + 1)) == n_clusters)\n",
    "\n",
    "    return m, conss_assignment, conss_capacity, cons_pmedian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1dd31f",
   "metadata": {},
   "source": [
    "## Use-Case 1: The Automatic Mode\n",
    "With the model built, we can now simply call the `optimize()` function and let GCG do its magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "selected-ticket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCG version 3.5.0 [GitHash: b54569ac6-dirty]\n",
      "Copyright (C) 2010-2021 Operations Research, RWTH Aachen University\n",
      "                        Konrad-Zuse-Zentrum fuer Informationstechnik Berlin (ZIB)\n",
      "\n",
      "SCIP version 8.0.0 [precision: 8 byte] [memory: block] [mode: optimized] [LP solver: SoPlex 6.0.0] [GitHash: 1af84b0617]\n",
      "Copyright (C) 2002-2021 Konrad-Zuse-Zentrum fuer Informationstechnik Berlin (ZIB)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j0/xrhjp0m56rq615f5c1vnhr300000gn/T/ipykernel_45959/1909260032.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mconss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_locations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdemands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapacities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/j0/xrhjp0m56rq615f5c1vnhr300000gn/T/ipykernel_45959/3991065140.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(n_locations, n_clusters, distances, demands, capacities)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"y_{j}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_locations\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"x_{i}_{j}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Hold different constraint types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "m, *conss = build_model(n_locations, n_clusters, distances, demands, capacities)\n",
    "m.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee4b2a1",
   "metadata": {},
   "source": [
    "## Use-Case 2: Exploring different Decompositions\n",
    "Above, we have seen GCG in its fully automatic mode. If we want to dig deeper, we can inspect the different decompositions that GCG detects. For that, we recreate the model and manually execute `presolve()` and `detect()` for the model. At this stage it is possible to list and visualize the found decompositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, *conss = build_model(n_locations, n_clusters, distances, demands, capacities)\n",
    "m.presolve()\n",
    "m.detect()\n",
    "\n",
    "decomps = m.listDecompositions()\n",
    "\n",
    "print(\"GCG found {} finnished decompositions.\".format(len(decomps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8ab777",
   "metadata": {},
   "source": [
    "### Inspecting Decompositions\n",
    "\n",
    "The call to `listDecompositions()` returns a list of `PartialDecomposition` objects. We can print a decomposition using the Python `print()` function to get a summary or access different fields directly.\n",
    "\n",
    "For a full overview of available methods, take a look at the online documentation for the `PartialDecomposition` class, or execute `help(d)` where `d` is a decomposition object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decomps)\n",
    "\n",
    "d = decomps[2]\n",
    "\n",
    "print(\n",
    "    f\"Decomp scores: {d.classicScore=:.04f}, {d.borderAreaScore=:.04f}, {d.maxWhiteScore=:.04f}, {d.maxForWhiteScore=:.04f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c54bd",
   "metadata": {},
   "source": [
    "### Visualizing Decompositions\n",
    "In addition, GCG can create graphical visualizations of decompositions. They can easily be displayed in a Jupyter nodebook like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb3d799",
   "metadata": {},
   "source": [
    "### Selecting Decompositions\n",
    "After this investigation, we decide that we like this particular decomposition. The following code orders GCG to select our decomposition instead of an automatic one. Then, the optimization process is started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-curtis",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.isSelected = True\n",
    "\n",
    "m.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f846322a",
   "metadata": {},
   "source": [
    "## Use-Case 3: Building a custom Decomposition\n",
    "In the previous use-cases we run GCG with automatically detected decompositions. This is useful if we do not know the exact structure of a model but still want to exploit GCG's decomposition approach.\n",
    "\n",
    "However, for our model we *do* know its structure. If you take another look at our `build_model()` method, you will notice that we store the created constraints in different variables based on their type. This is a typical approach when we want to specify a custom decomposition after building the model using the Python interface.\n",
    "\n",
    "In the following code, we recreate our model and use the different constraint types fo select constraints for reformulation and constraints for the Dantzig-Wolfe master problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, conss_assignment, conss_capacity, cons_pmedian = build_model(\n",
    "    n_locations, n_clusters, distances, demands, capacities\n",
    ")\n",
    "\n",
    "conss_master = conss_assignment + [cons_pmedian]\n",
    "conss_reform = conss_capacity\n",
    "\n",
    "pd = m.createPartialDecomposition()\n",
    "pd.fixConssToMaster(conss_master)\n",
    "\n",
    "for block, c in enumerate(conss_reform):\n",
    "    pd.fixConsToBlock(c, block)\n",
    "\n",
    "m.addPreexistingPartialDecomposition(pd)\n",
    "\n",
    "m.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d89973",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "With that, we have seen the most important features to use GCG as a solver through the Python interface. In a different example, we will take a look at how GCG can be extended using Python code."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f957d0bb3c15b1cedc83fb550c82626212cd1f371f4f3e6cf365a0241acd002e"
  },
  "kernelspec": {
   "display_name": "pygcgopt-env-docs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "96c411c10837af9498e15621947793a2592cd914a8b21febdb89cd6ff87a6197"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
